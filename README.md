# AI агент на LangGraph

Агент на Python строит динамическую цепочку обработки пользовательского запроса: определяет интенты через LLM, собирает маршрут `search → summarize → sentiment` (в нужном порядке) либо уходит в `fallback`, обрабатывает результаты и возвращает финальный ответ. Поддержан быстрый и глубокий режим поиска.

## Архитектура
- Состояние (`agent/models.py`): `user_query`, `detected_intents`, `intermediate_result`, `final_answer`, `next_intent_index`, `search_mode`.
- Граф (`agent/graph.py`): узлы `intent_recognition → router → {search|summarize|sentiment|fallback} → ... → finalize`. Маршрутизация по списку интентов; по завершении — `finalize`.
- Ноды (`agent/handlers.py`):
  - `intent_recognition_node`: LLM + нормализация интентов, безопасный фолбэк в `["none"]` при ошибке.
  - `search_tool_handler`: DuckDuckGo, при неудаче — LLM-заглушка; deep-режим скачивает топ-3 страниц и суммаризирует.
  - `summarize_handler`, `sentiment_handler`, `fallback_handler`: LLM-обработка с добавлением шага в общий ответ.
  - Все ноды обновляют общее состояние и сдвигают `next_intent_index`.
- Цепочки (`agent/chains.py`): промпты + LLM с structured output для интентов и поиска.
- CLI (`agent/cli.py`): цикл `input → graph.invoke → print`; парсит префиксы `mode=fast|mode=deep`, сбрасывает состояние на каждый запрос.

## Обоснование выбора LLM (GigaChat3-10B-A1.8B)
- Structured output: модель умеет отдавать валидный JSON (через `with_structured_output`), что критично для детекции интентов и генерации псевдо-выдачи поиска.
- Русский язык: обучена с хорошим покрытием RU, уверенно распознает формулировки интентов и выдает связные ответы на русском без дополнительного fine-tune.
- Детерминированность: температура 0 + стабильные системные промпты дают повторяемые маршруты (важно для тестов и фиксированного порядка интентов).
- Размер/скорость: 10B — достаточное качество для понимания запроса и коротких резюме, при этом быстрее и дешевле, чем более крупные модели, что важно в интерактивном CLI.
- Совместимость: OpenAI-совместимый чат без кастомных SDK, сразу работает с LangChain/LangGraph цепочками.

## Запуск
1) Установите зависимости: `pip install -r requirements.txt`  
2) Скопируйте `.env.example` в `.env` и укажите API ключ из cloud.ru (OpenAI-совместимый эндпоинт).  
3) Запустите: `python main.py`  
4) Примеры ввода:  
   - `Найди новости о ядерной энергетике и коротко перескажи`  
   - `mode=deep Найди новости о ядерной энергетике` (глубокий поиск с загрузкой страниц)  
   - `Определи тональность текста: "Мне всё понравилось"`  
   - `Сколько будет 2+2?` (уйдет в fallback)

## Соответствие ТЗ
- Интенты через LLM, поддержка множественных: `IntentPrediction` + нормализация в `intent_recognition_node`.
- Динамический граф LangGraph, порядок интентов учитывается; `search` перед `summarize`.
- Обработчики — отдельные ноды, обновляют общее состояние и итоговый ответ; `fallback` для `none`.
- Состояние на Pydantic с полями из ТЗ.
- Поток: `intent_recognition` → динамические обработчики → `finalize`.
- CLI-цикл, многократные запросы, состояние сбрасывается.
- Обработка ошибок: ленивый init LLM, фолбэки в поиске и интентах, логирование.
- Дополнение к ТЗ: `mode=deep` для загрузки HTML и суммаризации страниц.

## Результаты тестов (ручные прогоны)
Тесты прогонялись вручную после добавления deep-режима и фильтра интентов:

- `mode=deep Найди последние новости о морских коньках` → интенты `search` (LLM), реальная выдача DuckDuckGo, блок «Глубокое чтение страниц» с суммаризациями страниц: 

https://myseldon.com/ru/news/index/338980596
1. В «Москвариуме» впервые за 7 лет удалось получить потомство редких морских коньков вида Hippocampus reidi.
2. Разведение морских коньков в искусственной среде затруднено из-за низкой выживаемости потомства, но в «Москвариуме» удалось добиться успеха благодаря специальным условиям и питанию.
3. Для разведения морских коньков используется специализированный аквариум и особый живой корм из зоопланктона, который производится на собственной базе «Москвариума».
4. В дальнейшем специалисты планируют использовать полученный опыт для разведения других редких видов морских коньков, занесённых в Красную книгу России.

https://ria.ru/20240911/konki-1972012543.html
1. В Москвариуме впервые появилось потомство у редкого вида морских коньков Hippocampus reidi.
2. Специалисты океанариума активно занимаются размножением этого вида с 2017 года.
3. Из последнего приплода выжили около 25% детенышей, что ниже обычного показателя.
4. В планах Москвариума — использовать полученный опыт для размножения других видов морских коньков, занесенных в Красную книгу России.
5. Уникальная база по производству специализированного живого корма из зоопланктона способствует успешному размножению морских коньков в океанариуме.

https://www.newsinfo.ru/news/morskie-konki/875048/
1. Учёные исследуют популяцию морских коньков в Сетских каналах во Франции, чтобы определить их численность, ареал обитания и поведение.
2. Морские коньки служат индикаторами состояния окружающей среды, и их присутствие в каналах свидетельствует о здоровье экосистемы.
3. Проект направлен не только на научное изучение, но и на повышение осведомлённости жителей Сета о биоразнообразии и необходимости его сохранения.
4. Для достижения целей исследования планируется проведение лекций, экскурсий и образовательных программ.
5. Результаты исследования могут помочь разработать меры по защите морских коньков и сохранению уникальной экосистемы Сетских каналов.


- `Определи тональность: мне понравилось` → интент `sentiment`, ответ с оценкой «позитивная»:

Тональность: позитив

Обоснование: Слово "понравилось" выражает положительную оценку или удовлетворение от чего-либо, что указывает на позитивную тональность.


- `Сколько будет 2+2?` → нормализуется в `none`, ответ через `fallback` без поиска/суммаризации:

Ответ:
4

- `Привет, как дела?` → `none`, ответ через `fallback`: 

Ответ:
Привет! Всё хорошо, спасибо! А у тебя как?


## Известные ограничения
- Качество deep-режима зависит от доступности страниц и длины текста; суммаризация режет вход до 4000 символов.
- DuckDuckGo может возвращать капчу/блокировку; при этом используется LLM-заглушка.
- Без корректного API ключа LLM шаги деградируют в текст ошибок, но CLI продолжает работу.***
