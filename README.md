# AI агент на LangGraph

Агент на Python строит динамическую цепочку обработки пользовательского запроса: определяет интенты через LLM, собирает маршрут `search → summarize → sentiment` (в нужном порядке) либо уходит в `fallback`, обрабатывает результаты и возвращает финальный ответ. Поддержан быстрый и глубокий режим поиска.

## Архитектура
- Состояние (`agent/models.py`): `user_query`, `detected_intents`, `intermediate_result`, `final_answer`, `next_intent_index`, `search_mode`.
- Граф (`agent/graph.py`): узлы `intent_recognition → router → {search|summarize|sentiment|fallback} → ... → finalize`. Маршрутизация по списку интентов; по завершении — `finalize`.
- Ноды (`agent/handlers.py`):
  - `intent_recognition_node`: LLM + нормализация интентов, безопасный фолбэк в `["none"]` при ошибке.
  - `search_tool_handler`: DuckDuckGo, при неудаче — LLM-заглушка; deep-режим скачивает топ-3 страниц и суммаризирует.
  - `summarize_handler`, `sentiment_handler`, `fallback_handler`: LLM-обработка с добавлением шага в общий ответ.
  - Все ноды обновляют общее состояние и сдвигают `next_intent_index`.
- Цепочки (`agent/chains.py`): промпты + LLM с structured output для интентов и поиска.
- CLI (`agent/cli.py`): цикл `input → graph.invoke → print`; парсит префиксы `mode=fast|mode=deep`, сбрасывает состояние на каждый запрос.

## LLM и конфигурация
- Используется Groq (OpenAI-совместимый API): один API ключ, настраиваем приоритеты моделей по нодам графа.
- Конфигурация моделей и сэмплинга хранится в `llm_config.yaml`: список моделей с параметрами для каждой ноды (`intent`, `search`, `summarize`, `sentiment`, `fallback`).
- Параметры на модель: `temperature`, `top_p`, `max_tokens`; для интентов дефолт 0.0, для финального ответа можно выбрать более высокую температуру.
- Дефолтный набор: `llama-3.1-8b-instant`, `llama-3.3-70b-versatile`, `llama-4-scout-17b-16e-instruct`, `qwen/qwen3-32b`, `moonshotai/kimi-k2-instruct-0905` (см. `llm_config.yaml`).

## Запуск
1) Установите зависимости: `pip install -r requirements.txt`  
2) Скопируйте `.env.example` в `.env`, укажите `GROQ_API_KEY` (и при необходимости `GROQ_BASE_URL`), отредактируйте `llm_config.yaml`, если нужен другой набор моделей/параметров.  
3) Запустите: `python main.py`  
4) Примеры ввода:  
   - `Найди новости о ядерной энергетике и коротко перескажи`  
   - `Найди последние новости о морских коньках` (глубокий поиск по умолчанию)  
   - `Определи тональность текста: "Мне всё понравилось"`  
   - `Сколько будет 2+2?` (уйдет в fallback)

## Соответствие ТЗ
- Интенты через LLM, поддержка множественных: `IntentPrediction` + нормализация в `intent_recognition_node`.
- Динамический граф LangGraph, порядок интентов учитывается; `search` перед `summarize`.
- Обработчики — отдельные ноды, обновляют общее состояние и итоговый ответ; `fallback` для `none`.
- Состояние на Pydantic с полями из ТЗ.
- Поток: `intent_recognition` → динамические обработчики → `finalize`.
- CLI-цикл, многократные запросы, состояние сбрасывается.
- Обработка ошибок: ленивый init LLM, фолбэки в поиске и интентах, логирование.
- Дополнение к ТЗ: `mode=deep` для загрузки HTML и суммаризации страниц.

## Конфиг `llm_config.yaml`
- `defaults`: базовые `model`, `temperature`, `top_p`, `max_tokens`.
- `nodes`: списки моделей по нодам (`intent`, `search`, `summarize`, `sentiment`, `fallback`), перебираются по порядку до первой успешной.
- Пример дефолтного конфига лежит в корне (`llm_config.yaml`) и может быть переопределён через `LLM_CONFIG_PATH`.

## Переменные окружения
- `GROQ_API_KEY` (обязательно) - ключ Groq.  
- `GROQ_BASE_URL` - базовый URL, по умолчанию `https://api.groq.com/openai/v1`.  
- `LLM_CONFIG_PATH` - путь к yaml-конфигу моделей (по умолчанию `llm_config.yaml`).  

## Результаты тестов (ручные прогоны)
Тесты прогонялись вручную после добавления deep-режима и фильтра интентов:

- `mode=deep Найди последние новости о морских коньках` → интенты `search` (LLM), реальная выдача DuckDuckGo, блок «Глубокое чтение страниц» с суммаризациями страниц: 

https://myseldon.com/ru/news/index/338980596
1. В «Москвариуме» впервые за 7 лет удалось получить потомство редких морских коньков вида Hippocampus reidi.
2. Разведение морских коньков в искусственной среде затруднено из-за низкой выживаемости потомства, но в «Москвариуме» удалось добиться успеха благодаря специальным условиям и питанию.
3. Для разведения морских коньков используется специализированный аквариум и особый живой корм из зоопланктона, который производится на собственной базе «Москвариума».
4. В дальнейшем специалисты планируют использовать полученный опыт для разведения других редких видов морских коньков, занесённых в Красную книгу России.

https://ria.ru/20240911/konki-1972012543.html
1. В Москвариуме впервые появилось потомство у редкого вида морских коньков Hippocampus reidi.
2. Специалисты океанариума активно занимаются размножением этого вида с 2017 года.
3. Из последнего приплода выжили около 25% детенышей, что ниже обычного показателя.
4. В планах Москвариума — использовать полученный опыт для размножения других видов морских коньков, занесенных в Красную книгу России.
5. Уникальная база по производству специализированного живого корма из зоопланктона способствует успешному размножению морских коньков в океанариуме.

https://www.newsinfo.ru/news/morskie-konki/875048/
1. Учёные исследуют популяцию морских коньков в Сетских каналах во Франции, чтобы определить их численность, ареал обитания и поведение.
2. Морские коньки служат индикаторами состояния окружающей среды, и их присутствие в каналах свидетельствует о здоровье экосистемы.
3. Проект направлен не только на научное изучение, но и на повышение осведомлённости жителей Сета о биоразнообразии и необходимости его сохранения.
4. Для достижения целей исследования планируется проведение лекций, экскурсий и образовательных программ.
5. Результаты исследования могут помочь разработать меры по защите морских коньков и сохранению уникальной экосистемы Сетских каналов.
